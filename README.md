# ğŸ§  MoR vs Standard Transformer: A Comprehensive Implementation Study

<div align="center">

![Python](https://img.shields.io/badge/python-v3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-v2.0+-red.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Status](https://img.shields.io/badge/status-complete-brightgreen.svg)

**A deep dive into implementing and comparing Google DeepMind's Mixture of Recursions (MoR) architecture against standard transformers**

[ğŸ“Š Live Dashboard](your-streamlit-url) â€¢ [ğŸ“„ Research Paper](https://arxiv.org/pdf/2507.10524v1) â€¢ [ğŸ”— LinkedIn Post](your-linkedin-post-url)

</div>

---

## ğŸŒŸ Project Overview

This repository contains a comprehensive implementation and comparison study of **Mixture of Recursions (MoR)** architecture against standard transformers, including experiments on both custom models and Google's Gemma 3 270M. The project reveals important insights about the practical challenges and scale dependencies of cutting-edge architectural innovations.

### ğŸ¯ Key Findings

| Model | Final Loss | Training Time | Memory Usage | Parameters |
|-------|------------|---------------|--------------|------------|
| **Gemma Standard** | **6.03** âœ… | 239s | 3.24GB | 268M |
| **MoR Implementation** | 23.23 âŒ | 717s | 9-13GB | 130M |
| **MoR 400M** | 8.45 | 2500 steps | 8GB+ | 365M |
| **Standard 400M** | 7.82 | 2500 steps | 6GB | 405M |

> **ğŸ’¡ Key Insight**: MoR's benefits emerge primarily at billion+ parameter scales, with implementation complexity often outweighing theoretical advantages at smaller scales.

---

## ğŸ—ï¸ Architecture Implementations

### ğŸ”„ Mixture of Recursions (MoR)
- **Recursive Transformer Blocks**: Shared layers applied multiple times
- **Dynamic Token Routing**: ACT-style halting mechanism
- **KV Cache Optimization**: Selective caching for active tokens
- **Parameter Sharing**: Reduced unique parameters through recursion

### ğŸ›ï¸ Standard Transformer
- **Traditional Architecture**: Unique layers for each depth
- **Fixed Computation**: Uniform processing for all tokens
- **Standard Attention**: Multi-head self-attention mechanism
- **Linear Scaling**: Parameters scale with depth

### ğŸ¤– Gemma Integration
- **Base Model**: Google's Gemma 3 270M
- **MoR Adaptation**: Custom routing and recursion layers
- **Comparative Analysis**: Direct performance comparison

## ğŸš€ Quick Start

### Prerequisites
```bash
# Python 3.8+
pip install torch torchvision torchaudio
pip install transformers datasets
pip install streamlit plotly pandas numpy
pip install einops scipy
```

### ğŸƒâ€â™‚ï¸ Running the Experiments

#### 1. Train MoR Model
```bash
python train_mor.py
```

#### 2. Train Standard Transformer
```bash
python train_standard_transformer.py
```

#### 3. Train Gemma Models
```bash
# Set your Hugging Face token
export HF_TOKEN="your_token_here"

# Standard Gemma
python gemma.py

# Gemma with MoR
python gemma_mor_implementation.py
```

#### 4. Launch Interactive Dashboard
```bash
streamlit run visualize_training.py
```

---

## ğŸ“Š Interactive Dashboard Features

<div align="center">
<img src="images_results/dashboard_preview.png" alt="Dashboard Preview" width="800"/>
</div>

### ğŸ›ï¸ Dashboard Capabilities
- **ğŸ“ˆ Real-time Loss Comparison**: All 4 models side-by-side
- **ğŸ† Performance Leaderboard**: Ranked by multiple metrics
- **ğŸ—ï¸ Architecture Analysis**: Radar charts and specifications
- **âš¡ Training Efficiency**: Memory usage, convergence rates
- **ğŸ” Pairwise Comparisons**: Detailed model-vs-model analysis
- **ğŸ“Š Statistical Analysis**: Violin plots, correlations, distributions
- **ğŸ’¡ AI-Generated Insights**: Automated recommendations

### ğŸ¨ Visualization Gallery

| Loss Comparison | Architecture Radar | Performance Leaderboard |
|:---------------:|:------------------:|:----------------------:|
| ![Loss](images_results/loss_comparison.png) | ![Radar](images_results/architecture_radar.png) | ![Leaderboard](images_results/performance_leaderboard.png) |

---

## ğŸ”¬ Technical Deep Dive

### ğŸ§® MoR Implementation Details

```python
# Core MoR Architecture
class MoRLM(nn.Module):
    def __init__(self, cfg: MoRConfig):
        super().__init__()
        self.cfg = cfg
        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)
        self.pos_emb = nn.Embedding(cfg.max_seq_len, cfg.d_model)

        # Shared transformer blocks (key innovation)
        self.blocks = nn.ModuleList([
            TransformerBlock(cfg.d_model, cfg.n_heads, cfg.d_ff, cfg.dropout)
            for _ in range(cfg.n_layers_shared)
        ])

        # Dynamic routing mechanism
        self.router = HaltingRouter(cfg.d_model)
        self.norm_f = RMSNorm(cfg.d_model)
```

### ğŸ¯ Key Architectural Differences

| Aspect | MoR | Standard Transformer |
|--------|-----|---------------------|
| **Parameter Sharing** | âœ… Shared across recursions | âŒ Unique per layer |
| **Dynamic Computation** | âœ… Token-level routing | âŒ Fixed computation |
| **Memory Efficiency** | âœ… Selective KV caching | âŒ Full KV storage |
| **Training Complexity** | âŒ High (routing, halting) | âœ… Straightforward |
| **Scale Dependency** | âŒ Requires large scale | âœ… Works at any scale |

---

## ğŸ“ˆ Experimental Results

### ğŸ† Performance Summary

#### **Winner: Standard Architectures** ğŸ¥‡

**Gemma Standard (270M)**
- âœ… **Best Loss**: 6.03
- âœ… **Fastest Training**: 239s
- âœ… **Memory Efficient**: 3.24GB
- âœ… **Stable Convergence**

**Why MoR Underperformed:**
1. **Scale Dependency**: Requires 16.5e18 FLOPs (billion+ parameters)
2. **Routing Complexity**: Expert-choice mechanism introduces instability
3. **Memory Overhead**: Multiple expert networks increase memory pressure
4. **Implementation Challenges**: Complex architecture harder to optimize

### ğŸ“Š Detailed Metrics

<details>
<summary><b>ğŸ” Click to expand detailed results</b></summary>

#### MoR 400M Model
```json
{
  "final_loss": 8.45,
  "parameters": 365615105,
  "training_time": "2500 steps",
  "memory_usage": "8GB+",
  "architecture": "28 shared layers Ã— 3 recursions",
  "effective_depth": 84
}
```

#### Standard 400M Model
```json
{
  "final_loss": 7.82,
  "parameters": 405613568,
  "training_time": "2500 steps", 
  "memory_usage": "6GB",
  "architecture": "24 unique layers",
  "effective_depth": 24
}
```

#### Gemma MoR vs Standard
```json
{
  "gemma_standard": {
    "loss": 6.03,
    "time": "239s",
    "memory": "3.24GB",
    "status": "âœ… Winner"
  },
  "gemma_mor": {
    "loss": 23.23,
    "time": "717s", 
    "memory": "9-13GB",
    "status": "âŒ Underperformed"
  }
}
```

</details>

---

## ğŸ“ Research Insights

### ğŸ“š Based on Google DeepMind's Paper

The original research demonstrates MoR's effectiveness at massive scale:
- **118M MoR** outperforms **315M vanilla Transformer**
- **2x faster inference** with proper optimization
- **50% memory reduction** through selective caching
- **Parameter efficiency** through recursive weight sharing

### ğŸ” Our Implementation Findings

**Scale Matters**: 
- MoR benefits emerge at billion+ parameter scales
- Small models (270M) don't reach the critical threshold
- Infrastructure requirements are substantial

**Implementation Complexity**:
- Dynamic routing requires careful tuning
- Memory management is more complex
- Training stability needs attention

**Practical Considerations**:
- Standard architectures remain superior for resource-constrained scenarios
- MoR shows promise for large-scale deployments
- Hybrid approaches may offer best of both worlds

---

## ğŸ› ï¸ Advanced Usage

### ğŸ”§ Custom Configuration

```python
# MoR Configuration
config = MoRConfig(
    vocab_size=50257,
    d_model=1024,
    n_heads=16,
    d_ff=4096,
    n_layers_shared=28,    # Shared layers
    max_recursions=3,      # Recursion depth
    dropout=0.0,
    max_seq_len=256,
    kv_share_from_first=True,  # Memory optimization
    ponder_cost=0.01,          # Routing penalty
    tie_embeddings=True        # Parameter efficiency
)
```

### ğŸ“Š Custom Visualization

```python
# Launch dashboard with custom data
streamlit run visualize_training.py

# Or create custom plots
from visualize_training import create_comprehensive_loss_comparison
fig = create_comprehensive_loss_comparison(your_data)
fig.show()
```

### ğŸ¯ Hyperparameter Tuning

<details>
<summary><b>ğŸ”§ Recommended hyperparameters</b></summary>

#### For MoR Models:
```python
# Training
learning_rate = 2e-4
batch_size = 1
grad_accumulation = 16
warmup_steps = 100
max_iters = 2500

# Architecture  
ponder_cost = 0.01      # Encourage early halting
max_recursions = 3      # Balance depth vs efficiency
kv_share = True         # Memory optimization
```

#### For Standard Models:
```python
# Training
learning_rate = 1e-4
batch_size = 1
grad_accumulation = 8
scheduler = "CosineAnnealingLR"

# Architecture
n_layers = 24           # Match MoR effective depth
dropout = 0.0           # Minimal regularization
```

</details>

---

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

### ğŸ¯ Areas for Contribution
- **ğŸ”¬ New Architectures**: Implement other recursive/adaptive models
- **ğŸ“Š Visualizations**: Add new analysis charts and metrics
- **âš¡ Optimizations**: Improve training efficiency and memory usage
- **ğŸ§ª Experiments**: Test on different datasets and scales
- **ğŸ“š Documentation**: Enhance guides and tutorials

### ğŸš€ Getting Started
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“œ Citation

If you use this work in your research, please cite:

```bibtex
@misc{mor-vs-standard-2025,
  title={MoR vs Standard Transformer: A Comprehensive Implementation Study},
  author={Your Name},
  year={2025},
  url={https://github.com/yourusername/mor-vs-standard-transformer}
}

@article{mor-original-2025,
  title={Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation},
  author={Google DeepMind Team},
  journal={arXiv preprint arXiv:2507.10524},
  year={2025}
}
```

---

## ğŸ™ Acknowledgments

### ğŸŒŸ Special Thanks

- **Google DeepMind** for the groundbreaking MoR research and making it accessible
- **Google** for the Gemma 3 270M model and infrastructure
- **Hugging Face** for the transformers library and model hosting
- **PyTorch Team** for the excellent deep learning framework
- **Streamlit** for the amazing dashboard capabilities

### ğŸ“š Research Foundation

This work builds upon:
- [Mixture-of-Recursions Paper](https://arxiv.org/pdf/2507.10524v1) by Google DeepMind
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer
- [Gemma Technical Report](https://arxiv.org/abs/2403.08295) - Gemma Architecture

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ”— Links & Resources

### ğŸŒ Project Links
- **ğŸ“Š [Live Dashboard](your-streamlit-url)** - Interactive analysis
- **ğŸ“± [LinkedIn Post](your-linkedin-post-url)** - Project summary
- **ğŸ“§ [Contact](mailto:your-email)** - Get in touch

### ğŸ“š Research Resources
- **[Original MoR Paper](https://arxiv.org/pdf/2507.10524v1)** - Google DeepMind
- **[Gemma Model Card](https://huggingface.co/google/gemma-3-270m)** - Hugging Face
- **[PyTorch Documentation](https://pytorch.org/docs/)** - Framework docs

### ğŸ› ï¸ Technical Resources
- **[Streamlit Docs](https://docs.streamlit.io/)** - Dashboard framework
- **[Plotly Documentation](https://plotly.com/python/)** - Visualization library
- **[Transformers Library](https://huggingface.co/docs/transformers/)** - Model implementations

---

<div align="center">

### ğŸŒŸ Star this repository if you found it helpful!

**Built with â¤ï¸ for the AI research community**

*Pushing the boundaries of what's possible in efficient AI architectures*

</div>